<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration>

<!--
    Example logback.xml demonstrating the ElasticAppender configuration.

    This appender sends logs directly to Elasticsearch data streams via the Bulk API,
    mimicking Elastic Agent's filestream behavior without writing to an intermediate log file.

    The resulting data stream name will be: logs-myapp-production
    (constructed from dataStreamType-dataStreamDataset-dataStreamNamespace)
-->

<configuration>
    <include resource="org/springframework/boot/logging/logback/defaults.xml"/>
    <include resource="org/springframework/boot/logging/logback/console-appender.xml"/>

    <springProperty name="elasticUrl"       source="elastic.apm.server_url"  defaultValue="https://localhost:9200"/>
    <springProperty name="elasticDataset"   source="elastic.apm.service_name" defaultValue="generic"/>
    <springProperty name="elasticNamespace" source="elastic.apm.environment"  defaultValue="default"/>

    <appender name="ELASTIC" class="fr.insee.rmes.logger.elastic.ElasticAppender">

        <!--
            ECS log formatting delegated to Spring Boot's StructuredLogEncoder.
        -->
        <encoder class="org.springframework.boot.logging.logback.StructuredLogEncoder">
            <format>ecs</format>
        </encoder>

        <url>${elasticUrl}</url>

        <!--
            Data stream configuration.
            The final data stream name = {type}-{dataset}-{namespace}
            Elasticsearch must have a matching index template (the built-in
            logs-*-* template handles this automatically).
        -->
        <dataStreamType>logs</dataStreamType>
        <dataStreamDataset>${elasticDataset}</dataStreamDataset>
        <dataStreamNamespace>${elasticNamespace}</dataStreamNamespace>

        <!--
            Batching & performance tuning.
            Events are buffered in a queue and sent in bulk batches.
        -->
        <batchSize>100</batchSize>              <!-- Max events per bulk request -->
        <flushIntervalMillis>5000</flushIntervalMillis>  <!-- Flush every 5s even if batch not full -->
        <maxQueueSize>10000</maxQueueSize>       <!-- Drop events when queue exceeds this -->

        <!-- HTTP timeouts -->
        <connectTimeoutMillis>5000</connectTimeoutMillis>
        <readTimeoutMillis>30000</readTimeoutMillis>
        <maxRetries>3</maxRetries>

    </appender>


    <root level="INFO">
        <appender-ref ref="CONSOLE"/>
        <appender-ref ref="ELASTIC" />
    </root>

</configuration>